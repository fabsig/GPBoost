---
title: "GPBoost: Combining Tree-Boosting with Gaussian Process and Mixed Effects Models"
subtitle: "Examples on how to use the GPBoost library from R and Python"
author: "Fabio Sigrist"
output:
  html_document:
    # code_folding: hide
    css: buttonstyle.css # htmlpreview.github.io does not correctly display this on github, also not the code_folding option
    number_sections: true
    toc: true
    toc_float: true
    theme: flatly
---

<!-- The following java script option works, but htmlpreview.github.io does not correctly display it on github-->
<script src="hideOutput.js"></script>
<!-- See https://stackoverflow.com/questions/37755037/how-to-add-code-folding-to-output-chunks-in-rmarkdown-html-documents -->

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.width=7, fig.height=5)
library(reticulate)
# py_install("gpboost",pip=TRUE)
# py_install("shap",pip=TRUE)
run_python = TRUE # Not run for publication
run_cv = FALSE # Not run for publication
run_python_cv = run_python & run_cv
export_data_from_R_to_python = TRUE
run_group_re = TRUE
cache = FALSE # Set to TRUE to save image files
small_figures = FALSE
if (small_figures) {
  # Smaller figures to save space
  fig.width = 6.5
  fig.height = 4.5
} else {
  # Default values
  fig.width = 7
  fig.height = 5
}
```

In this document, we illustrate how the GPBoost R and Python packages can be used to train models, make predictions, interpret models, and choose tuning parameters. Further, we also compare the GPBoost algorithm to classical tree-boosting and to plain linear Gaussian process regression.

# Methodology Background
Both tree-boosting and Gaussian processes are techniques that achieve **state-of-the-art predictive accuracy**. Besides this, **tree-boosting** has the following advantages: 

* Automatic modeling of non-linearities, discontinuities, and complex high-order interactions
* Robust to outliers in and multicollinearity among predictor variables
* Scale-invariance to monotone transformations of the predictor variables
* Automatic handling of missing values in predictor variables

**Gaussian process** models have the following advantage:

* Probabilistic predictions which allows for uncertainty quantification
* Modeling of dependency which, among other things, can allow for more efficient learning of the fixed effects / regression function

For the GPBoost algorithm, it is assumed that the **response variable (label) $y$ is the sum of a non-linear mean function $F(X)$ and so-called random effects $Zb$**:
$$ y = F(X) + Zb + \xi,$$
where $\xi$ is an independent error term and $X$ are predictor variables (aka covariates or features).

The **random effects** can consists of

- Gaussian processes (including random coefficient processes)
- Grouped random effects (including nested, crossed, and random coefficient effects)
- A sum of the above

The model is trained using the **GPBoost algorithm, where trainings means learning the covariance parameters** of the random effects and the **mean function F(X) using a tree ensemble**. In brief, the GPBoost algorithm is a boosting algorithm that iteratively learns the covariance parameters and adds a tree to the ensemble of trees using a gradient and/or a Newton boosting step. In the GPBoost library, covariance parameters can be learned using (accelerated) gradient descent or Fisher scoring. Further, trees are learned using the [LightGBM](https://github.com/microsoft/LightGBM/) library. See [Sigrist (2020)](http://arxiv.org/abs/2004.02653) for more details.


# Combining Tree-Boosting and Gaussian Process Models 
In the following, we start with an example where the random part of the model consists of a Gaussian process. After this, we also give a brief example when using grouped random effects instead of a Gaussian process.

## Data
First, you need to load your data. For simplicity and illustrational purposes, we use simulated data here. In the following, we simulate data and illustrate the data in the figure below.

***In R***
<div class="fold s">
```{r simulate_data, results='hide', message=F, cache=cache}
set.seed(1)
# Simulate Gaussian process: training and test data (the latter on a grid for visualization)
cov_function <- "exponential"
sigma2_1 <- 0.35 # marginal variance of GP
rho <- 0.1 # range parameter
sigma2 <- 0.1 # error variance
n <- 200 # number of training samples
nx <- 50 # test data: number of grid points on each axis
# training locations (exclude upper right rectangle)
coords <- matrix(runif(2)/2,ncol=2, dimnames=list(NULL,paste0("s_",1:2)))
while (dim(coords)[1]<n) {
  coord_i <- runif(2) 
  if (!(coord_i[1]>=0.6 & coord_i[2]>=0.6)) {
    coords <- rbind(coords,coord_i)
  }
}
# test locations (rectangular grid)
s_1 <- s_2 <- rep((1:nx)/nx,nx)
for(i in 1:nx) s_2[((i-1)*nx+1):(i*nx)]=i/nx
coords_test <- cbind(s_1=s_1,s_2=s_2)
n_all <- nx^2 + n # total number of data points 
D <- as.matrix(dist(rbind(coords_test,coords))) # distance matrix
if(cov_function=="exponential"){
  Sigma <- exp(-D/rho)+diag(1E-10,n_all)
}else if (cov_function=="gaussian"){
  Sigma <- exp(-(D/rho)^2)+diag(1E-10,n_all)
}
C <- t(chol(Sigma))
b_all <- sqrt(sigma2_1)*C%*%rnorm(n_all)
b <- b_all[(nx^2+1):n_all] # training data GP
# Mean function. Use two predictor variables of which only one has an effect for easy visualization
f1d <- function(x) sin(3*pi*x) + (1 + 3 * pmax(0,x-0.5)/(x-0.5)) - 3
X <- matrix(runif(2*n), ncol=2, dimnames=list(NULL,paste0("Covariate_",1:2)))
F_X <- f1d(X[,1]) # mean
xi <- sqrt(sigma2) * rnorm(n) # simulate error term
y <- F_X + b + xi # observed data
# test data
x <- seq(from=0,to=1,length.out=nx^2)
x[x==0.5] = 0.5 + 1E-10
X_test <- cbind(Covariate_1=x, Covariate_2=rep(0,nx^2))
y_test <- f1d(X_test[,1]) + b_all[1:nx^2] + sqrt(sigma2) * rnorm(nx^2)
```
</div>

```{python export_data, include=FALSE, eval=export_data_from_R_to_python}
coords = r.coords
X = r.X
y = r.y
coords_test = r.coords_test
X_test = r.X_test
y_test = r.y_test
```

***In Python***
<div class="fold s o">
```{python simulate_data_python, collapse=T, message=F, cache=cache, eval=!export_data_from_R_to_python}
import numpy as np
np.random.seed(1)
# Simulate Gaussian process: training and test data (the latter on a grid for visualization)
sigma2_1 = 0.35  # marginal variance of GP
rho = 0.1  # range parameter
sigma2 = 0.1  # error variance
n = 200  # number of training samples
nx = 50 # test data: number of grid points on each axis
# training locations (exclude upper right rectangle)
coords = np.column_stack(
  (np.random.uniform(size=1)/2, np.random.uniform(size=1)/2))
  while coords.shape[0] < n:
    coord_i = np.random.uniform(size=2)
    if not (coord_i[0] >= 0.6 and coord_i[1] >= 0.6):
      coords = np.vstack((coords,coord_i))
# test locations (rectangular grid)
s_1 = np.ones(nx * nx)
s_2 = np.ones(nx * nx)
for i in range(nx):
  for j in range(nx):
    s_1[j * nx + i] = (i + 1) / nx
    s_2[i * nx + j] = (i + 1) / nx
coords_test = np.column_stack((s_1, s_2))
n_all = nx**2 + n # total number of data points 
coords_all = np.vstack((coords_test,coords))
D = np.zeros((n_all, n_all))  # distance matrix
for i in range(0, n_all):
  for j in range(i + 1, n_all):
    D[i, j] = np.linalg.norm(coords_all[i, :] - coords_all[j, :])
    D[j, i] = D[i, j]
Sigma = sigma2_1 * np.exp(-D / rho) + np.diag(np.zeros(n_all) + 1e-10)
C = np.linalg.cholesky(Sigma)
b_all = C.dot(np.random.normal(size=n_all))
b = b_all[(nx*nx):n_all] # training data GP
# Mean function. Use two predictor variables of which only one has an effect for easy visualization
def f1d(x):
  return np.sin(3*np.pi*x) + (1 + 3 * np.maximum(np.zeros(len(x)),x-0.5)/(x-0.5)) - 3
X = np.random.rand(n, 2)
F_X = f1d(X[:, 0]) # mean
xi = np.sqrt(sigma2) * np.random.normal(size=n)  # simulate error term
y = F_X + b + xi  # observed data
# test data
x = np.linspace(0,1,nx**2)
x[x==0.5] = 0.5 + 1e-10
X_test = np.column_stack((x,np.zeros(nx**2)))
y_test = f1d(X_test[:, 0]) + b_all[0:(nx**2)] + np.sqrt(sigma2) * np.random.normal(size=(nx**2))
```
</div>


```{r plot_data, echo=F, results='hide', message=F, cache=cache, warning=FALSE, fig.align='center', fig.cap='Illustration of data: mean function F(X) used for simulation, "true" latent Gaussian process b, and observed data y (bottom plots)', fig.width=0.9*fig.width, fig.height=fig.height}
library(ggplot2)
library(viridis)
library(gridExtra)
x <- seq(from=0,to=1,length.out=200)
plot1 <- ggplot(data=data.frame(x=x,f=f1d(x)), aes(x=x,y=f)) + geom_line(size=1.5, color="darkred") +
ggtitle("Mean function") + xlab("X_1") + ylab("F(X)")
plot2 <- ggplot(data = data.frame(s_1=coords_test[,1],s_2=coords_test[,2],b=b_all[1:nx^2]),aes(x=s_1,y=s_2,color=b)) + 
geom_point(size=2, shape=15) + scale_color_viridis(option = "B") + ggtitle("\"True\" Gaussian process (GP)")
plot3 <- ggplot(data=data.frame(x=X[,1],y=y), aes(x=x,y=y)) + geom_point() + 
geom_line(data=data.frame(x=x,f=f1d(x)), aes(x=x,y=f), size=1.5, color="darkred") +
ggtitle("Observed data vs. predictor variable") + xlab("X_1") + ylab("y")
plot4 <- ggplot(data = data.frame(s_1=coords[,1],s_2=coords[,2],y=y),aes(x=s_1,y=s_2,color=y)) + 
geom_point(size=3) + scale_color_viridis(option = "B") + ggtitle("Observed data and locations")
grid.arrange(plot1, plot2, plot3, plot4, ncol=2, widths = c(2.5,3))
```


## Training
**Training a model** is done by 

1. **Specifying the GP / random effects model as a `GPModel`** and, optionally, setting parameters for the optimization of the covariance parameters
2. **Training the GPBoost model by calling `gpboost` or, equivalently, `gpb.train` and passing the `GPModel` as an argument**

***In R***
```{r training, results='hold', message=F, cache=cache, eval=T}
# Create Gaussian process / random effects model
library(gpboost)
gp_model <- GPModel(gp_coords = coords, cov_function = "exponential")
# Train model
bst <- gpboost(data = X, label = y,
               gp_model = gp_model,
               nrounds = 247, learning_rate = 0.01,
               max_depth = 3, min_data_in_leaf = 10, num_leaves = 2^10,
               objective = "regression_l2", verbose = 0)
# Estimated covariance parameters of the GPModel
print("Estimated covariance parameters:")
summary(gp_model)
print("True values:")
print(c(sigma2,sigma2_1,rho))
```

***In Python***
<div class="fold s o">
```{python training_python, collapse=T, message=F, cache=cache, eval=run_python}
import gpboost as gpb
import numpy as np
# Create Gaussian process / random effects model and Dataset
gp_model = gpb.GPModel(gp_coords=coords, cov_function="exponential")
data_train = gpb.Dataset(X, y)
params = { 'objective': 'regression_l2', 'learning_rate': 0.01,
'max_depth': 3, 'min_data_in_leaf': 10, 'num_leaves': 2**10, 'verbose': 0 }
bst = gpb.train(params=params, train_set=data_train,
gp_model=gp_model, num_boost_round=247)
print("Estimated covariance parameters:")
gp_model.summary()
```
</div>


## Prediction
**Prediction is done by calling the `predict` function** and passing the predictor variables for the tree ensemble and the features that define the Gaussian process or random effect, i.e. the prediction locations in our case. Note that the predictions for the tree ensemble and the Gaussian process are returned separately. I.e., one needs to sum them to obtain a single point prediction.

***In R***
```{r prediction, results='hold', message=F, cache=cache, fig.align='center', fig.cap='Prediction: Predicted (posterior) mean and prediction uncertainty (=standard deviation) of GP as well as predicted mean function F(X).'}
# Make predictions
pred <- predict(bst, data = X_test, gp_coords_pred = coords_test, predict_var = TRUE)
# Sum the predictions of the trees and the GP
y_pred <- pred$fixed_effect + pred$random_effect_mean
# Calculate mean square error
MSE_GPBoost <- mean((y_pred-y_test)^2)
print(paste0("Mean square error: ", MSE_GPBoost)) # 0.3942886
```
Plot predictions
<div class="fold s o">
```{r plot_prediction, results='hide', fig.keep = "none", message=F, cache=cache}
# Plot predictions
plot_data <- data.frame(s_1=coords_test[,1],s_2=coords_test[,2],
                        mean=pred$random_effect_mean,
                        sd=sqrt(pred$random_effect_cov),
                        b=b_all[1:nx^2])
plot_data_fixed_effect <- data.frame(rbind(cbind(x=X_test[,1],f=pred$fixed_effect,F=rep("pred",dim(X_test)[1])),
                                           cbind(x=x,f=f1d(x),F=rep("true",length(x)))))
plot_data_fixed_effect$f <- as.numeric(plot_data_fixed_effect$f)
plot_data_fixed_effect$x <- as.numeric(plot_data_fixed_effect$x)
plot5 <- ggplot(data = plot_data, aes(x=s_1,y=s_2,color=mean)) +
  geom_point(size=2, shape=15) + scale_color_viridis(option = "B") + ggtitle("Predicted GP mean")
plot6 <- ggplot(data = plot_data, aes(x=s_1,y=s_2,color=sd)) +
  geom_point(size=2, shape=15) + scale_color_viridis(option = "B") + 
  labs(title="Predicted GP standard deviation")
plot7 <- ggplot(data=plot_data_fixed_effect, aes(x=x,y=f,group=F,color=F)) + geom_line(size=1.5) +
  ggtitle("Learned and true F(X)") + xlab("X_1") + ylab("y") + scale_color_manual(values=c("black","darkred"))
plot8 <- ggplot(data = plot_data, aes(x=s_1,y=s_2,color=b)) + 
  geom_point(size=2, shape=15) + scale_color_viridis(option = "B") + ggtitle("\"True\" GP and obs. locations") + 
  geom_point(data = data.frame(s_1=coords[,1],s_2=coords[,2],y=y),aes(x=s_1,y=s_2),size=3,col="white", alpha=1, shape=43)
grid.arrange(plot8, plot5, plot6, plot7, ncol=2)
```
</div>
```{r show_plot_prediction, echo=FALSE, results='hold', message=F, cache=cache, fig.align='center', fig.cap='Prediction: Predicted (posterior) mean and prediction uncertainty (=standard deviation) of GP as well as predicted mean function F(X).', fig.width=fig.width, fig.height=fig.height}
grid.arrange(plot8, plot5, plot6, plot7, ncol=2)
```

***In Python***
<div class="fold s o">
```{python prediction_python, collapse=T, message=F, cache=cache, eval=run_python}
pred = bst.predict(data=X_test, gp_coords_pred=coords_test, predict_var=True)
y_pred = pred['fixed_effect'] + pred['random_effect_mean']
print("Mean square error (MSE): " + str(np.mean((y_pred-y_test)**2)))
```
</div>

## Model interpretation
A trained model can be interpreted using various tools. Besides classical feature importance measures and partial dependence plots, SHAP values and dependence plots can be created as follows. *Note that for the R package, you need version 0.4.3 or latter for creating these SHAP plots.*

***In R***
```{r interpretation, echo=TRUE, warning=FALSE, results='hold', message=F, cache=cache, fig.align='center', fig.width=fig.width, fig.height=0.5*fig.width}
library("SHAPforxgboost")
p1 <- shap.plot.summary.wrap1(bst, X = X)
shap_long <- shap.prep(bst, X_train = X)
p2 <- shap.plot.dependence(data_long = shap_long, x = "Covariate_1",
                           color_feature = "Covariate_2", smooth = FALSE)
grid.arrange(p1, p2, ncol=2)
```

***In Python***
<div class="fold s o">
```{python interpretation_python, results='hide', collapse=T, message=F, cache=cache, eval=run_python}
import shap
shap_values = shap.TreeExplainer(bst).shap_values(X)
shap.summary_plot(shap_values, X)
shap.dependence_plot("Feature 0", shap_values, X)
```
</div>


## Parameter tuning
Boosting with trees as base learners has several tuning parameters. Arguably the most important one is the number of boosting iterations (=number of trees). Other tuning parameters include the learning rate, the maximal tree depth, the minimal number of samples per leaf, the number of leaves, and others. For instance, these can be chosen using a random or deterministic grid search and k-fold cross-validation as shown in the following. A computationally cheaper alternative to full k-fold cross-validation is to pass a validation data set to the parameter tuning function. See the [**Python parameter tuning demo**](https://github.com/fabsig/GPBoost/tree/master/examples/python-guide/parameter_tuning.py) and [**R parameter tuning demo**](https://github.com/fabsig/GPBoost/tree/master/R-package/demo/parameter_tuning.R) for more details. *Note that you need GPBoost version 0.4.3 or latter for using the `grid.search.tune.parameters` function.*

***In R***
```{r cv, results='show', message=F, cache=cache, eval=run_cv}
# Create random effects model and datasets
gp_model <- GPModel(gp_coords = coords, cov_function = "exponential")
gp_model$set_optim_params(params=list("optimizer_cov" = "gradient_descent"))
# Use (Nesterov accelerated) gradient descent instead of the default option 
#   Fisher scoring (aka natural gradient descent) since gradient descent is more
#   robust for "bad choices" of tuning parameters. In these situations the 
#   negative log-likelihood of the GP model can be very flat which can cause problems for Fisher scoring.
dtrain <- gpb.Dataset(data = X, label = y)
# Candidate parameter grid
param_grid = list("learning_rate" = c(0.1,0.05,0.01),
                  "min_data_in_leaf" = c(5,10,20,50),
                  "max_depth" = c(1,3,5,10))
# Other parameters not contained in the grid of tuning parameters
params <- list(objective = "regression_l2", verbose = 0, "num_leaves" = 2^10)
# Use random grid search and cross-validation. Set 'num_try_random=NULL' to use deterministic grid search
opt_params <- gpb.grid.search.tune.parameters(
  param_grid = param_grid,
  params = params,
  num_try_random = 20,
  nfold = 4,
  data = dtrain,
  gp_model = gp_model,
  verbose_eval = 1,
  nrounds = 1000,
  early_stopping_rounds = 5,
  eval = "l2")
# Found the following parameters:
# ***** New best score (0.397766105827059) found for the following parameter combination: learning_rate: 0.01, min_data_in_leaf: 10, max_depth: 3, nrounds: 247
```

***In Python***
<div class="fold s o">
```{python cv_python, collapse=T, message=F, cache=cache, eval=run_python_cv}
# Create random effects model and Dataset
gp_model = gpb.GPModel(gp_coords=coords, cov_function="exponential")
gp_model.set_optim_params(params={"optimizer_cov": "gradient_descent"})
data_train = gpb.Dataset(X, y)
# Candidate parameter grid
param_grid = {'learning_rate': [0.1,0.05,0.01], 'min_data_in_leaf': [5,10,20,50],
'max_depth': [1,3,5,10,20]}
# Other parameters not contained in the grid of tuning parameters
params = { 'objective': 'regression_l2', 'verbose': 0, 'num_leaves': 2**17 }
# Use a deterministic grid search and cross-validation. Set 'num_try_random' to use a random grid search
opt_params = gpb.grid_search_tune_parameters(
  param_grid=param_grid,
  params=params,
  num_try_random=20,
  nfold=4,
  gp_model=gp_model,
  use_gp_model_for_validation=True,
  train_set=data_train,
  verbose_eval=1,
  num_boost_round=1000, 
  early_stopping_rounds=5,
  seed=1,
  metrics='l2')
print("Best number of iterations: " + str(opt_params['best_iter']))
print("Best score: " + str(opt_params['best_score']))
print("Best parameters: " + str(opt_params['best_params']))
```
</div>

## Comparison to plain tree-boosting and a linear Gaussian process model
In the following, we compare the GPBoost algorithm to plain gradient tree-boosting with a squared loss, to a linear Gaussian process model, and to random forest. For gradient boosting and random forest, we add the coordinates to the predictor variables for the trees. For the linear Gaussian process, we include the predictor variables in a linear regression term. As the results below show, **plain gradient tree-boosting, random forest, and a linear Gaussian process model result in considerably larger mean square errors (MSE) compared to the GPBoost algorithm**.

***In R***
```{r comparison, results='hold', message=F, cache=cache, eval=T}
# 1. Linear Gaussian process model
# Add an intercept term to the model matrix
X1 <- cbind(Intercept=rep(1,n),X)
gp_model <- fitGPModel(gp_coords = coords, cov_function = "exponential",
                       y = y, X = X1)
X_test1 <- cbind(rep(1,dim(X_test)[1]),X_test)
print("Fitted linear Gaussian process model:")
summary(gp_model)
y_pred_linGP <- predict(gp_model, gp_coords_pred = coords_test,X_pred = X_test1)
MSE_lin <- mean((y_pred_linGP$mu-y_test)^2)
print(paste0("MSE of linear Gaussian process model: ", MSE_lin)) # 1.306599

# 2. Gradient tree-boosting with a squared loss
# Add the coordinates to the predictor variables
XC <- cbind(X,coords)
X_testC <- cbind(X_test,coords_test)
# Choose tuning parameters
# dtrain <- gpb.Dataset(data = XC, label = y)
# set.seed(1)
# opt_params <- gpb.grid.search.tune.parameters(
#   param_grid = param_grid,
#   params = params,
#   num_try_random = NULL,
#   nfold = 4,
#   data = dtrain,
#   verbose_eval = 1,
#   nrounds = 1000,
#   early_stopping_rounds = 5,
#   eval = "l2")
# Found the following parameters:
# ***** New best score (0.450492863373992) found for the following parameter combination: learning_rate: 0.05, min_data_in_leaf: 20, max_depth: 10, nrounds: 176
bst <- gpboost(data = XC, label = y,
               nrounds = 176, learning_rate = 0.05,
               max_depth = 10, min_data_in_leaf = 20,
               objective = "regression_l2", verbose = 0)
pred_l2boost <- predict(bst, data = X_testC)
MSE_l2boost <- mean((pred_l2boost-y_test)^2)
print(paste0("MSE of plain gradient tree-boosting: ", MSE_l2boost)) # 0.555046

# 3. Random forest
library(randomForest)
colnames(XC) <- colnames(X_testC) <- paste0("V",rep(1:dim(XC)[2]))
# Choose tuning parameters
# library(caret)
# control <- trainControl(method="cv", number=4, search="grid")
# tunegrid <- expand.grid(.mtry=c(1:4))
# set.seed(1)
# rf_gridsearch <- train(y~., data=data.frame(y=y,XC), method="rf", metric="RMSE", tuneGrid=tunegrid, trControl=control)
# print(rf_gridsearch)
# Found the following parameters: mtry = 3
set.seed(1)
rf <- randomForest(y ~ ., data = XC, ntree=1000, mtry=3)
# plot(rf) # check "convergence"
pred_rf <- predict(rf, newdata = X_testC) ## predicted labels
MSE_rf <- mean((pred_rf-y_test)^2)
print(paste0("MSE of random forest: ", MSE_rf)) # 0.5730892

print("Compare root mean square errors of different methods:")
RMSEs <- sqrt(c(GPBoost=MSE_GPBoost, Lin_GP=MSE_lin, L2Boost=MSE_l2boost, RF=MSE_rf))
print(RMSEs)
```

# Combining Tree-Boosting and Grouped Random Effects
In the following, we show how a non-linear mixed effects model can be trained using `gpboost`. We use a random effects model that includes two crossed grouped random effects as well as a random slope. We first simulate data and then show how to train a GPBoost model and make predictions.
***In R***
<div class="fold s">
```{r simulate_group_data, results='hide', message=F, cache=cache}
# Simulate data
set.seed(1)
n <- 1000 # number of samples
m <- 25 # number of categories / levels for grouping variable
n_obs_gr <- n/m # number of sampels per group
group <- rep(1,n) # grouping variable for first random effect
for(i in 1:m) group[((i-1)*n/m+1):(i*n/m)] <- i
group2 <- rep(1,n) # grouping variable for second crossed random effect
for(i in 1:m) group2[(1:n_obs_gr)+n_obs_gr*(i-1)] <- 1:n_obs_gr
sigma2_1 <- 1^2 # variance of first random effect
sigma2_2 <- 0.5^2 # variance of second random effect
sigma2_3 <- 0.75^2 # variance of random slope for first random effect
sigma2 <- 0.5^2 # error variance
# incidence matrixces relating grouped random effects to samples
Z1 <- model.matrix(rep(1,n) ~ factor(group) - 1)
Z2 <- model.matrix(rep(1,n)~factor(group2)-1) # incidence matrix for second random effect
x_rand_slope <- runif(n) # covariate data for random slope
Z3 <- diag(x_rand_slope) %*% Z1 # incidence matrix for random slope for first random effect
b1 <- sqrt(sigma2_1) * rnorm(m) # simulate random effects
b2 <- sqrt(sigma2_2) * rnorm(n_obs_gr) # second random effect
b3 <- sqrt(sigma2_3) * rnorm(m) # random slope for first random effect
b <- Z1%*%b1 + Z2%*%b2 + Z3%*%b3 # sum of all random effects
# Function for non-linear mean. Two predictor variables of which only one has an effect
f1d <- function(x) 2*(1/(1+exp(-(x-0.5)*20)))
X <- matrix(runif(2*n),ncol=2)
F_X <- f1d(X[,1]) # mean
xi <- sqrt(sigma2) * rnorm(n) # simulate error term
y <- F_X + b + xi # observed data
# test data
X_test <- cbind(seq(from=0,to=1,length.out=n),rep(0,n))
group_test = rep(1,n)
group2_test = rep(1,n)
x_rand_slope_test = rep(0,n)
```
</div>

***In R***
```{r train_group_model, results='hold', message=F, cache=cache, eval=run_group_re}
# Create random effects model
gp_model <- GPModel(group_data = cbind(group,group2),
                    group_rand_coef_data = x_rand_slope,
                    ind_effect_group_rand_coef = 1)# the random slope is for the first random effect

# Parameter tuning
dtrain <- gpb.Dataset(data = X, label = y)
param_grid = list("learning_rate" = c(0.1,0.05,0.01),
                  "min_data_in_leaf" = c(5,10,20,50),
                  "max_depth" = c(1,3,5,10))
params <- list(objective = "regression_l2", verbose = 0, "num_leaves" = 2^10)
set.seed(1)
# opt_params <- gpb.grid.search.tune.parameters(param_grid = param_grid,
#                                               params = params,
#                                               num_try_random = 20,
#                                               nfold = 4,
#                                               data = dtrain,
#                                               gp_model = gp_model,
#                                               verbose_eval = 1,
#                                               nrounds = 1000,
#                                               early_stopping_rounds = 5,
#                                               eval = "l2")
# Found the following parameters:
# ***** New best score (0.303743116584455) found for the following parameter combination: learning_rate: 0.1, min_data_in_leaf: 10, max_depth: 1, nrounds: 16

# Train model
bst <- gpboost(data = X, label = y,
               gp_model = gp_model,
               nrounds = 16, learning_rate = 0.1,
               max_depth = 1, min_data_in_leaf = 10,
               objective = "regression_l2", verbose = 0)
print("Estimated variance parameters:")
summary(gp_model)
print("True values:")
print(c(sigma2,sigma2_1,sigma2_2,sigma2_3))

# Make predictions
pred <- predict(bst, data = X_test, group_data_pred = cbind(group_test,group2_test),
                group_rand_coef_data_pred = x_rand_slope_test)

```