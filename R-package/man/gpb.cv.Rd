% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/gpb.cv.R
\name{gpb.cv}
\alias{gpb.cv}
\title{Main CV logic for GPBoost}
\usage{
gpb.cv(params = list(), data, nrounds = 100, nfold = 4, label = NULL,
  obj = NULL, gp_model = NULL, use_gp_model_for_validation = FALSE,
  fit_GP_cov_pars_OOS = FALSE, train_gp_model_cov_pars = TRUE,
  weight = NULL, eval = NULL, verbose = 1, record = TRUE,
  eval_freq = 1L, showsd = FALSE, stratified = TRUE, folds = NULL,
  init_model = NULL, colnames = NULL, categorical_feature = NULL,
  early_stopping_rounds = NULL, callbacks = list(), reset_data = FALSE,
  ...)
}
\arguments{
\item{params}{List of parameters (many of them tuning paramters), see Parameters.rst for more information. A few key parameters:
\itemize{
    \item{learning_rate}{ The learning rate, also called shrinkage or damping parameter 
    (default = 0.1). An important tuning parameter for boosting. Lower values usually 
    lead to higher predictive accuracy but more boosting iterations are needed }
    \item{num_leaves}{ Number of leaves in a tree. Tuning parameter for 
    tree-boosting (default = 127)}
    \item{min_data_in_leaf}{ Minimal number of samples per leaf. Tuning parameter for 
    tree-boosting (default = 20)}
    \item{max_depth}{ Maximal depth of a tree. Tuning parameter for tree-boosting (default = no limit)}
    \item{leaves_newton_update}{ Set this to TRUE to do a Newton update step for the tree leaves 
    after the gradient step. Applies only to Gaussian process boosting (GPBoost algorithm)}
    \item{train_gp_model_cov_pars}{ If TRUE, the covariance parameters of the Gaussian process 
    are stimated in every boosting iterations, 
    otherwise the GPModel parameters are not estimated. In the latter case, you need to 
    either esimate them beforehand or provide the values via 
    the 'init_cov_pars' parameter when creating the GPModel (default = TRUE).}
    \item{use_gp_model_for_validation}{ If TRUE, the Gaussian process is also used 
    (in addition to the tree model) for calculating predictions on the validation data 
    (default = FALSE)}
    \item{use_nesterov_acc}{ Set this to TRUE to do boosting with Nesterov acceleration. 
    Can currently only be used for tree_learner = "serial" (default option)}
    \item{nesterov_acc_rate}{ Acceleration rate for momentum step in case Nesterov accelerated 
    boosting is used (default = 0.5)}
    \item{boosting}{ Boosting type. \code{"gbdt"} or \code{"dart"}. Only "gpdt" allows for 
    doing Gaussian process boosting}
    \item{num_threads}{ Number of threads. For the best speed, set this to
                       the number of real CPU cores, not the number of threads (most
                       CPU using hyper-threading to generate 2 threads per CPU core).}
}}

\item{data}{A matrix with covariate (feature) data or a \code{gpb.Dataset} object containing covariate data for training}

\item{nrounds}{Number of boosting iterations (= number of trees). This is the most important tuning parameter for boosting}

\item{nfold}{The original dataset is randomly partitioned into \code{nfold} equal size subsamples.}

\item{label}{Vector of response variables / labels, used if \code{data} is not an \code{\link{gpb.Dataset}}}

\item{obj}{Objective function, can be character or custom objective function (default = "regression_l2"). Examples include
\code{regression_l2}, \code{regression_l1}, \code{huber},
\code{binary}, \code{lambdarank}, \code{multiclass}, \code{multiclass}. Currently only "regression_l2" supports Gaussian process boosting.}

\item{gp_model}{A \code{GPModel} object that contains the random effects (Gaussian process and / or grouped random effects) model. Can currently only be used for objective = "regression"}

\item{use_gp_model_for_validation}{Boolean (default = FALSE). If TRUE, the Gaussian process is also used (in addition to the tree model) for calculating predictions on the validation data}

\item{fit_GP_cov_pars_OOS}{Boolean (default = FALSE). If TRUE, the covariance parameters of the 
GPModel model are estimated using the out-of-sample (OOS) predictions 
on the validation data using the optimal number of iterations (after performing the CV)}

\item{train_gp_model_cov_pars}{Boolean (default = TRUE). If TRUE, the covariance parameters of the Gaussian process are estimated in every boosting iterations, 
otherwise the GPModel parameters are not estimated. In the latter case, you need to either esimate them beforehand or provide the values via 
the 'init_cov_pars' parameter when creating the GPModel}

\item{weight}{Vector of weights for samples (default = NULL). This is currently not supported for Gaussian process boosting (i.e. it only affects the trees and the Gaussian process or random effects model ignores the weights)}

\item{eval}{Evaluation function, can be (a list of) character or custom eval function}

\item{verbose}{Verbosity for output, if <= 0, also will disable the print of evaluation during training}

\item{record}{Boolean, TRUE will record iteration message to \code{booster$record_evals}}

\item{eval_freq}{Evaluation output frequency, only effect when verbose > 0}

\item{showsd}{\code{boolean}, whether to show standard deviation of cross validation}

\item{stratified}{a \code{boolean} indicating whether sampling of folds should be stratified
by the values of outcome labels.}

\item{folds}{\code{list} provides a possibility to use a list of pre-defined CV folds
(each element must be a vector of test fold's indices). When folds are supplied,
the \code{nfold} and \code{stratified} parameters are ignored.}

\item{init_model}{Path of model file of \code{gpb.Booster} object, will continue training from this model}

\item{colnames}{Feature (covariate) names, if not null, will use this to overwrite the names in dataset}

\item{categorical_feature}{List of str or int
type int represents index,
type str represents feature names}

\item{early_stopping_rounds}{int
Activates early stopping.
Requires at least one validation data and one metric
If there's more than one, will check all of them except the training data
Returns the model with (best_iter + early_stopping_rounds)
If early stopping occurs, the model will have 'best_iter' field}

\item{callbacks}{list of callback functions
List of callback functions that are applied at each iteration.}

\item{reset_data}{Boolean, setting it to TRUE (not the default value) will transform the booster model into a predictor model which frees up memory and the original datasets}

\item{...}{Other parameters, see Parameters.rst for more information.}
}
\value{
a trained model \code{gpb.CVBooster}.
}
\description{
Cross validation logic used by GPBoost
}
\examples{
\dontrun{
require(gpboost)

#--------------------Cross validation for tree-boosting without GP or random effects----------------
# Non-linear function for simulation
f1d <- function(x) 1.7*(1/(1+exp(-(x-0.5)*20))+0.75*x)
x <- seq(from=0,to=1,length.out=200)
plot(x,f1d(x),type="l",lwd=2,col="red",main="Mean function")
# Function that simulates data. Two covariates of which only one has an effect
sim_data <- function(n){
  X=matrix(runif(2*n),ncol=2)
  # mean function plus noise
  y=f1d(X[,1])+rnorm(n,sd=0.1)
  return(list(X=X,y=y))
}
# Simulate data
n <- 1000
set.seed(1)
data <- sim_data(2 * n)
dtrain <- gpb.Dataset(data$X[1:n,], label = data$y[1:n])
nrounds <- 100
params <- list(learning_rate = 0.1,
               max_depth = 6,
               min_data_in_leaf = 5,
               objective = "regression_l2")

print("Running cross validation with mean squared error")
bst <- gpb.cv(params = params,
              data = dtrain,
              nrounds = nrounds,
              nfold = 10,
              eval = "l2",
              early_stopping_rounds = 5)
print(paste0("Optimal number of iterations: ", bst$best_iter))

print("Running cross validation with mean absolute error")
bst <- gpb.cv(params = params,
              data = dtrain,
              nrounds = nrounds,
              nfold = 10,
              eval = "l1",
              early_stopping_rounds = 5)
print(paste0("Optimal number of iterations: ", bst$best_iter))


#--------------------Custom loss function----------------
# Cross validation can also be done with a cutomized loss function
# Define custom loss (quantile loss)
quantile_loss <- function(preds, dtrain) {
  alpha <- 0.95
  labels <- getinfo(dtrain, "label")
  y_diff <- as.numeric(labels-preds)
  dummy <- ifelse(y_diff<0,1,0)
  quantloss <- mean((alpha-dummy)*y_diff)
  return(list(name = "quant_loss", value = quantloss, higher_better = FALSE))
}

print("Running cross validation, with cutomsized loss function (quantile loss)")
bst <- gpb.cv(params = params,
              data = dtrain,
              nrounds = nrounds,
              nfold = 10,
              eval = quantile_loss,
              early_stopping_rounds = 5)
print(paste0("Optimal number of iterations: ", bst$best_iter))


#--------------------Combine tree-boosting and grouped random effects model----------------
# Simulate data
f1d <- function(x) 1.7*(1/(1+exp(-(x-0.5)*20))+0.75*x)
set.seed(1)
n <- 1000 # number of samples
X <- matrix(runif(2*n),ncol=2)
y <- f1d(X[,1]) # mean
# Add grouped random effects
m <- 25 # number of categories / levels for grouping variable
group <- rep(1,n) # grouping variable
for(i in 1:m) group[((i-1)*n/m+1):(i*n/m)] <- i
sigma2_1 <- 1^2 # random effect variance
sigma2 <- 0.1^2 # error variance
# incidence matrix relating grouped random effects to samples
Z1 <- model.matrix(rep(1,n) ~ factor(group) - 1)
b1 <- sqrt(sigma2_1) * rnorm(m) # simulate random effects
eps <- Z1 \%*\% b1
xi <- sqrt(sigma2) * rnorm(n) # simulate error term
y <- y + eps + xi # add random effects and error to data

# Create random effects model and dataset
gp_model <- GPModel(group_data = group)
dtrain <- gpb.Dataset(X, label = y)
params <- list(learning_rate = 0.05,
               max_depth = 6,
               min_data_in_leaf = 5,
               objective = "regression_l2")

print("Running cross validation for GPBoost model")
bst <- gpb.cv(params = params,
              data = dtrain,
              gp_model = gp_model,
              nrounds = 100,
              nfold = 10,
              eval = "l2",
              early_stopping_rounds = 5)
print(paste0("Optimal number of iterations: ", bst$best_iter))

# Include random effect predictions for validation (observe the lower test error)
gp_model <- GPModel(group_data = group)
print("Running cross validation for GPBoost model and use_gp_model_for_validation = TRUE")
bst <- gpb.cv(params = params,
              data = dtrain,
              gp_model = gp_model,
              use_gp_model_for_validation = TRUE,
              nrounds = 100,
              nfold = 10,
              eval = "l2",
              early_stopping_rounds = 5)
print(paste0("Optimal number of iterations: ", bst$best_iter))
}
}
