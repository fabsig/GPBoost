% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/gpboost.R
\name{gpboost}
\alias{gpboost}
\title{Train a GPBoost model}
\usage{
gpboost(data, label = NULL, nrounds = 100, obj = NULL, gp_model = NULL,
  train_gp_model_cov_pars = TRUE, params = list(), valids = list(),
  early_stopping_rounds = NULL, use_gp_model_for_validation = FALSE,
  weight = NULL, verbose = 1, ...)
}
\arguments{
\item{data}{A matrix with covariate (=feature) data or a \code{gpb.Dataset} object for training}

\item{label}{Vector of response variables / labels, used if \code{data} is not an \code{\link{gpb.Dataset}}}

\item{nrounds}{Number of boosting iterations (= number of trees). This is the most important tuning parameter for boosting}

\item{obj}{Objective function, can be character or custom objective function (default = "regression_l2"). Examples include
\code{regression_l2}, \code{regression_l1}, \code{huber},
\code{binary}, \code{lambdarank}, \code{multiclass}, \code{multiclass}. Currently only "regression_l2" supports Gaussian process boosting.}

\item{gp_model}{A \code{GPModel} object that contains the random effects (Gaussian process and / or grouped random effects) model. Can currently only be used for objective = "regression"}

\item{train_gp_model_cov_pars}{Boolean (default = TRUE). If TRUE, the covariance parameters of the Gaussian process are estimated in every boosting iterations, 
otherwise the GPModel parameters are not estimated. In the latter case, you need to either esimate them beforehand or provide the values via 
the 'init_cov_pars' parameter when creating the GPModel}

\item{params}{List of parameters (many of them tuning paramters), see Parameters.rst for more information. A few key parameters:
\itemize{
    \item{learning_rate}{ The learning rate, also called shrinkage or damping parameter 
    (default = 0.1). An important tuning parameter for boosting. Lower values usually 
    lead to higher predictive accuracy but more boosting iterations are needed }
    \item{num_leaves}{ Number of leaves in a tree. Tuning parameter for 
    tree-boosting (default = 127)}
    \item{min_data_in_leaf}{ Minimal number of samples per leaf. Tuning parameter for 
    tree-boosting (default = 20)}
    \item{max_depth}{ Maximal depth of a tree. Tuning parameter for tree-boosting (default = no limit)}
    \item{leaves_newton_update}{ Set this to TRUE to do a Newton update step for the tree leaves 
    after the gradient step. Applies only to Gaussian process boosting (GPBoost algorithm)}
    \item{train_gp_model_cov_pars}{ If TRUE, the covariance parameters of the Gaussian process 
    are stimated in every boosting iterations, 
    otherwise the GPModel parameters are not estimated. In the latter case, you need to 
    either esimate them beforehand or provide the values via 
    the 'init_cov_pars' parameter when creating the GPModel (default = TRUE).}
    \item{use_gp_model_for_validation}{ If TRUE, the Gaussian process is also used 
    (in addition to the tree model) for calculating predictions on the validation data 
    (default = FALSE)}
    \item{use_nesterov_acc}{ Set this to TRUE to do boosting with Nesterov acceleration. 
    Can currently only be used for tree_learner = "serial" (default option)}
    \item{nesterov_acc_rate}{ Acceleration rate for momentum step in case Nesterov accelerated 
    boosting is used (default = 0.5)}
    \item{boosting}{ Boosting type. \code{"gbdt"} or \code{"dart"}. Only "gpdt" allows for 
    doing Gaussian process boosting}
    \item{num_threads}{ Number of threads. For the best speed, set this to
                       the number of real CPU cores, not the number of threads (most
                       CPU using hyper-threading to generate 2 threads per CPU core).}
}}

\item{valids}{A list of \code{gpb.Dataset} objects, used as validation data}

\item{early_stopping_rounds}{int
Activates early stopping.
Requires at least one validation data and one metric
If there's more than one, will check all of them except the training data
Returns the model with (best_iter + early_stopping_rounds)
If early stopping occurs, the model will have 'best_iter' field}

\item{use_gp_model_for_validation}{Boolean (default = FALSE). If TRUE, the Gaussian process is also used (in addition to the tree model) for calculating predictions on the validation data}

\item{weight}{Vector of weights for samples (default = NULL). This is currently not supported for Gaussian process boosting (i.e. it only affects the trees and the Gaussian process or random effects model ignores the weights)}

\item{verbose}{Verbosity for output, if <= 0, also will disable the print of evaluation during training}

\item{...}{Additional arguments passed to \code{\link{gpb.train}}. See the documentation (help) of \code{\link{gpb.train}}.}
}
\value{
a trained booster model \code{gpb.Booster}.
}
\description{
Simple interface for training a GPBoost model.
}
\examples{
## SEE ALSO THE HELP OF 'gpb.train' FOR MORE EXAMPLES
\dontrun{
library(gpboost)

#--------------------Example without a Gaussian process or random effects model--------------
# Non-linear function for simulation
f1d <- function(x) 1.7*(1/(1+exp(-(x-0.5)*20))+0.75*x)
x <- seq(from=0,to=1,length.out=200)
plot(x,f1d(x),type="l",lwd=2,col="red",main="Mean function")
# Function that simulates data. Two covariates of which only one has an effect
sim_data <- function(n){
  X=matrix(runif(2*n),ncol=2)
  # mean function plus noise
  y=f1d(X[,1])+rnorm(n,sd=0.1)
  return(list(X=X,y=y))
}
# Simulate data
n <- 1000
set.seed(1)
data <- sim_data(2 * n)
Xtrain <- data$X[1:n,]
ytrain <- data$y[1:n]
Xtest <- data$X[1:n + n,]
ytest <- data$y[1:n + n]

# Train model
print("Train boosting model")
bst <- gpboost(data = Xtrain,
               label = ytrain,
               nrounds = 40,
               learning_rate = 0.1,
               max_depth = 6,
               min_data_in_leaf = 5,
               objective = "regression_l2",
               verbose = 0)

# Make predictions and compare fit to truth
x <- seq(from=0,to=1,length.out=200)
Xtest_plot <- cbind(x,rep(0,length(x)))
pred_plot <- predict(bst, data = Xtest_plot)
plot(x,f1d(x),type="l",ylim = c(-0.25,3.25), col = "red", lwd = 2,
     main = "Comparison of true and fitte value")
lines(x,pred_plot, col = "blue", lwd = 2)
legend("bottomright", legend = c("truth", "fitted"),
       lwd=2, col = c("red", "blue"), bty = "n")

# Prediction accuracy       
pred <- predict(bst, data = Xtest)
err <- mean((ytest-pred)^2)
print(paste("test-RMSE =", err))


#--------------------Combine tree-boosting and Gaussian process model----------------
# Simulate data
# Function for non-linear mean. Two covariates of which only one has an effect
f1d <- function(x) 1.7*(1/(1+exp(-(x-0.5)*20))+0.75*x)
set.seed(2)
n <- 200 # number of samples
X <- matrix(runif(2*n),ncol=2)
y <- f1d(X[,1]) # mean
# Add Gaussian process
sigma2_1 <- 1^2 # marginal variance of GP
rho <- 0.1 # range parameter
sigma2 <- 0.1^2 # error variance
coords <- cbind(runif(n),runif(n)) # locations (=features) for Gaussian process
D <- as.matrix(dist(coords))
Sigma <- sigma2_1*exp(-D/rho)+diag(1E-20,n)
C <- t(chol(Sigma))
b_1 <- rnorm(n) # simulate random effect
eps <- C \%*\% b_1
xi <- sqrt(sigma2) * rnorm(n) # simulate error term
y <- y + eps + xi # add random effects and error to data
# Create Gaussian process model
gp_model <- GPModel(gp_coords = coords, cov_function = "exponential")
# Default optimizer for covariance parameters is Fisher scoring.
# This can be changed as follows:
# re_params <- list(optimizer_cov = "gradient_descent", lr_cov = 0.05,
#                   use_nesterov_acc = TRUE, acc_rate_cov = 0.5)
# gp_model$set_optim_params(params=re_params)

# Train model
print("Train boosting with Gaussian process model")
bst <- gpboost(data = X,
               label = y,
               gp_model = gp_model,
               nrounds = 8,
               learning_rate = 0.1,
               max_depth = 6,
               min_data_in_leaf = 5,
               objective = "regression_l2",
               verbose = 0)
print("Estimated random effects model")
summary(gp_model)

# Make predictions
set.seed(1)
ntest <- 5
Xtest <- matrix(runif(2*ntest),ncol=2)
# prediction locations (=features) for Gaussian process
coords_test <- cbind(runif(ntest),runif(ntest))/10
pred <- predict(bst, data = Xtest, gp_coords_pred = coords_test,
                predict_cov_mat =TRUE)
print("Predicted (posterior) mean of GP")
pred$random_effect_mean
print("Predicted (posterior) covariance matrix of GP")
pred$random_effect_cov
print("Predicted fixed effect from tree ensemble")
pred$fixed_effect
}
}
