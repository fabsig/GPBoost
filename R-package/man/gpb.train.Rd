% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/gpb.train.R
\name{gpb.train}
\alias{gpb.train}
\title{Main function for training with GPBoost}
\usage{
gpb.train(data, nrounds = 100, obj = NULL, gp_model = NULL,
  train_gp_model_cov_pars = TRUE, params = list(), valids = list(),
  early_stopping_rounds = NULL, use_gp_model_for_validation = FALSE,
  eval = NULL, verbose = 1, record = TRUE, eval_freq = 1,
  init_model = NULL, colnames = NULL, categorical_feature = NULL,
  callbacks = list(), reset_data = FALSE, ...)
}
\arguments{
\item{data}{A \code{gpb.Dataset} object. Some functions, such as \code{gpboost} and \code{gpb.cv}, 
allow you to pass other types of data such as \code{matrix} and then separately 
supply \code{label} as argument.}

\item{nrounds}{Number of boosting iterations (= number of trees). This is the most important tuning parameter for boosting}

\item{obj}{Objective function, can be character or custom objective function (default = "regression_l2"). Examples include
\code{regression_l2}, \code{regression_l1}, \code{huber},
\code{binary}, \code{lambdarank}, \code{multiclass}, \code{multiclass}. Currently only "regression_l2" supports Gaussian process boosting.}

\item{gp_model}{A \code{gpb.GPModel} object that contains the random effect (Gaussian process) model. Can currently only be used for objective = "regression"}

\item{train_gp_model_cov_pars}{Boolean (default = TRUE). If TRUE, the covariance parameters of the Gaussian process are estimated in every boosting iterations, 
otherwise the GPModel parameters are not estimated. In the latter case, you need to either esimate them beforehand or provide the values via 
the 'init_cov_pars' parameter when creating the GPModel}

\item{params}{List of parameters (many of them tuning paramters), see Parameters.rst for more information. A few key parameters:
\itemize{
    \item{learning_rate}{ The learning rate, also called shrinkage or damping parameter 
    (default = 0.1). An important tuning parameter for boosting. Lower values usually 
    lead to higher predictive accuracy but more boosting iterations are needed }
    \item{num_leaves}{ Number of leaves in a tree. Tuning parameter for 
    tree-boosting (default = 127)}
    \item{min_data_in_leaf}{ Minimal number of samples per leaf. Tuning parameter for 
    tree-boosting (default = 20)}
    \item{max_depth}{ Maximal depth of a tree. Tuning parameter for tree-boosting (default = no limit)}
    \item{leaves_newton_update}{ Set this to TRUE to do a Newton update step for the tree leaves 
    after the gradient step. Applies only to Gaussian process boosting (GPBoost algorithm)}
    \item{train_gp_model_cov_pars}{ If TRUE, the covariance parameters of the Gaussian process 
    are stimated in every boosting iterations, 
    otherwise the GPModel parameters are not estimated. In the latter case, you need to 
    either esimate them beforehand or provide the values via 
    the 'init_cov_pars' parameter when creating the GPModel (default = TRUE).}
    \item{use_gp_model_for_validation}{ If TRUE, the Gaussian process is also used 
    (in addition to the tree model) for calculating predictions on the validation data 
    (default = FALSE)}
    \item{use_nesterov_acc}{ Set this to TRUE to do boosting with Nesterov acceleration. 
    Can currently only be used for tree_learner = "serial" (default option)}
    \item{nesterov_acc_rate}{ Acceleration rate for momentum step in case Nesterov accelerated 
    boosting is used (default = 0.5)}
    \item{boosting}{ Boosting type. \code{"gbdt"} or \code{"dart"}. Only "gpdt" allows for 
    doing Gaussian process boosting}
    \item{num_threads}{ Number of threads. For the best speed, set this to
                       the number of real CPU cores, not the number of threads (most
                       CPU using hyper-threading to generate 2 threads per CPU core).}
}}

\item{valids}{A list of \code{gpb.Dataset} objects, used as validation data}

\item{early_stopping_rounds}{int
Activates early stopping.
Requires at least one validation data and one metric
If there's more than one, will check all of them except the training data
Returns the model with (best_iter + early_stopping_rounds)
If early stopping occurs, the model will have 'best_iter' field}

\item{use_gp_model_for_validation}{Boolean (default = FALSE). If TRUE, the Gaussian process is also used (in addition to the tree model) for calculating predictions on the validation data}

\item{eval}{Evaluation function, can be (a list of) character or custom eval function}

\item{verbose}{Verbosity for output, if <= 0, also will disable the print of evaluation during training}

\item{record}{Boolean, TRUE will record iteration message to \code{booster$record_evals}}

\item{eval_freq}{Evaluation output frequency, only effect when verbose > 0}

\item{init_model}{Path of model file of \code{gpb.Booster} object, will continue training from this model}

\item{colnames}{Feature (covariate) names, if not null, will use this to overwrite the names in dataset}

\item{categorical_feature}{List of str or int
type int represents index,
type str represents feature names}

\item{callbacks}{list of callback functions
List of callback functions that are applied at each iteration.}

\item{reset_data}{Boolean, setting it to TRUE (not the default value) will transform the booster model into a predictor model which frees up memory and the original datasets}

\item{...}{Other parameters, see Parameters.rst for more information.}
}
\value{
a trained booster model \code{gpb.Booster}.
}
\description{
Main function for training with GPBoost
}
\examples{
\dontrun{
library(gpboost)

#--------------------Example without a Gaussian process or random effects model--------------
# Non-linear function for simulation
f1d <- function(x) 1.7*(1/(1+exp(-(x-0.5)*20))+0.75*x)
x <- seq(from=0,to=1,length.out=200)
plot(x,f1d(x),type="l",lwd=2,col="red",main="Mean function")
# Function that simulates data. Two covariates of which only one has an effect
sim_data <- function(n){
  X=matrix(runif(2*n),ncol=2)
  # mean function plus noise
  y=f1d(X[,1])+rnorm(n,sd=0.1)
  return(list(X=X,y=y))
}
# Simulate data
n <- 1000
set.seed(1)
data <- sim_data(2 * n)
Xtrain <- data$X[1:n,]
ytrain <- data$y[1:n]
dtrain <- gpb.Dataset(data = data$X[1:n,], label = data$y[1:n])
Xtest <- data$X[1:n + n,]
ytest <- data$y[1:n + n]

# Train model
print("Train boosting model")
bst <- gpb.train(data = dtrain,
                 nrounds = 40,
                 learning_rate = 0.1,
                 max_depth = 6,
                 min_data_in_leaf = 5,
                 objective = "regression_l2",
                 verbose = 0)

# Make predictions and compare fit to truth
x <- seq(from=0,to=1,length.out=200)
Xtest_plot <- cbind(x,rep(0,length(x)))
pred_plot <- predict(bst, data = Xtest_plot)
plot(x,f1d(x),type="l",ylim = c(-0.25,3.25), col = "red", lwd = 2,
     main = "Comparison of true and fitted value")
lines(x,pred_plot, col = "blue", lwd = 2)
legend("bottomright", legend = c("truth", "fitted"),
       lwd=2, col = c("red", "blue"), bty = "n")
       
# Prediction accuracy       
pred <- predict(bst, data = Xtest)
err <- mean((ytest-pred)^2)
print(paste("test-RMSE =", err))


#--------------------Using validation set-------------------------
# valids is a list of gpb.Dataset, each of them is tagged with a name
dtest <- gpb.Dataset.create.valid(dtrain, data = Xtest, label = ytest)
valids <- list(test = dtest)

# To train with valids, use gpb.train, which contains more advanced features
# valids allows us to monitor the evaluation result on all data in the list
print("Training using gpb.train with validation data ")
bst <- gpb.train(data = dtrain,
                 nrounds = 100,
                 learning_rate = 0.1,
                 max_depth = 6,
                 min_data_in_leaf = 5,
                 objective = "regression_l2",
                 verbose = 1,
                 valids = valids,
                 early_stopping_rounds = 5)
print(paste0("Optimal number of iterations: ", bst$best_iter))

# We can change evaluation metrics, or use multiple evaluation metrics
print("Train using gpb.train with multiple validation metrics")
bst <- gpb.train(data = dtrain,
                 nrounds = 100,
                 learning_rate = 0.1,
                 max_depth = 6,
                 min_data_in_leaf = 5,
                 objective = "regression_l2",
                 verbose = 1,
                 valids = valids,
                 eval = c("l2", "l1"),
                 early_stopping_rounds = 5)
print(paste0("Optimal number of iterations: ", bst$best_iter))


#--------------------Nesterov accelerated boosting-------------------------
dtrain <- gpb.Dataset(data = Xtrain, label = ytrain)
dtest <- gpb.Dataset.create.valid(dtrain, data = Xtest, label = ytest)
valids <- list(test = dtest)
print("Training using gpb.train with Nesterov acceleration")
bst <- gpb.train(data = dtrain,
                 nrounds = 100,
                 learning_rate = 0.01,
                 max_depth = 6,
                 min_data_in_leaf = 5,
                 objective = "regression_l2",
                 verbose = 1,
                 valids = valids,
                 early_stopping_rounds = 5,
                 use_nesterov_acc = TRUE)
# Compare fit to truth
x <- seq(from=0,to=1,length.out=200)
Xtest_plot <- cbind(x,rep(0,length(x)))
pred_plot <- predict(bst, data = Xtest_plot)
plot(x,f1d(x),type="l",ylim = c(-0.25,3.25), col = "red", lwd = 2,
     main = "Comparison of true and fitted value")
lines(x,pred_plot, col = "blue", lwd = 2)
legend("bottomright", legend = c("truth", "fitted"),
       lwd=2, col = c("red", "blue"), bty = "n")

              
#--------------------Combine tree-boosting and grouped random effects model----------------
## SEE ALSO THE HELP OF 'gpboost' FOR MORE EXAMPLES (in particular a Gaussian process example)
# Simulate data
# Function for non-linear mean. Two covariates of which only one has an effect
f1d <- function(x) 1.7*(1/(1+exp(-(x-0.5)*20))+0.75*x)
x=seq(from=0,to=1,length.out=200)
plot(x,f1d(x),type="l",lwd=2,col="red",main="Mean function")
set.seed(1)
n <- 1000 # number of samples
X=matrix(runif(2*n),ncol=2)
y <- f1d(X[,1]) # mean
# Add grouped random effects
m <- 25 # number of categories / levels for grouping variable
group <- rep(1,n) # grouping variable
for(i in 1:m) group[((i-1)*n/m+1):(i*n/m)] <- i
sigma2_1 <- 1^2 # random effect variance
sigma2 <- 0.1^2 # error variance
# incidence matrix relating grouped random effects to samples
Z1 <- model.matrix(rep(1,n) ~ factor(group) - 1)
b1 <- sqrt(sigma2_1) * rnorm(m) # simulate random effects
eps <- Z1 \%*\% b1
xi <- sqrt(sigma2) * rnorm(n) # simulate error term
y <- y + eps + xi # add random effects and error to data


#--------------------Training----------------
# Create random effects model
gp_model <- GPModel(group_data = group)
# The properties of the optimizer for the Gaussian process or 
#   random effects model can be set as follows
# re_params <- list(trace=TRUE,optimizer_cov="gradient_descent",
#                   lr_cov = 0.05, use_nesterov_acc = TRUE)
re_params <- list(optimizer_cov="fisher_scoring")
gp_model$set_optim_params(params=re_params)
print("Train boosting with random effects model")
bst <- gpboost(data = X,
               label = y,
               gp_model = gp_model,
               nrounds = 16,
               learning_rate = 0.05,
               max_depth = 6,
               min_data_in_leaf = 5,
               objective = "regression_l2",
               verbose = 0)

# Same thing using the gpb.train function
print("Training with gpb.train")
dtrain <- gpb.Dataset(data = X, label = y)
bst <- gpb.train(data = dtrain,
                 gp_model = gp_model,
                 nrounds = 16,
                 learning_rate = 0.05,
                 max_depth = 6,
                 min_data_in_leaf = 5,
                 objective = "regression_l2",
                 verbose = 0)

print("Estimated random effects model")
summary(gp_model)


#--------------------Prediction--------------
group_test <- 1:m
x <- seq(from=0,to=1,length.out=m)
Xtest <- cbind(x,rep(0,length(x)))
pred <- predict(bst, data = Xtest, group_data_pred = group_test)
# Compare fit to truth: random effects
pred_random_effect <- pred$random_effect_mean
plot(b1, pred_random_effect, xlab="truth", ylab="predicted",
     main="Comparison of true and predicted random effects")
abline(a=0,b=1)
# Compare fit to truth: fixed effect (mean function)
pred_mean <- pred$fixed_effect
plot(x,f1d(x),type="l",ylim = c(-0.25,3.25), col = "red", lwd = 2,
     main = "Comparison of true and fitted value")
points(x,pred_mean, col = "blue", lwd = 2)
legend("bottomright", legend = c("truth", "fitted"),
       lwd=2, col = c("red", "blue"), bty = "n")


#--------------------Using validation set-------------------------
set.seed(1)
train_ind <- sample.int(n,size=900)
dtrain <- gpb.Dataset(data = X[train_ind,], label = y[train_ind])
dtest <- gpb.Dataset.create.valid(dtrain, data = X[-train_ind,], label = y[-train_ind])
valids <- list(test = dtest)
gp_model <- GPModel(group_data = group[train_ind])

print("Training with validation data and use_gp_model_for_validation = FALSE ")
bst <- gpb.train(data = dtrain,
                 gp_model = gp_model,
                 nrounds = 100,
                 learning_rate = 0.05,
                 max_depth = 6,
                 min_data_in_leaf = 5,
                 objective = "regression_l2",
                 verbose = 1,
                 valids = valids,
                 early_stopping_rounds = 5,
                 use_gp_model_for_validation = FALSE)
print(paste0("Optimal number of iterations: ", bst$best_iter,
             ", best test error: ", bst$best_score))

# Include random effect predictions for validation (observe the lower test error)
gp_model <- GPModel(group_data = group[train_ind])
gp_model$set_prediction_data(group_data_pred = group[-train_ind])
print("Training with validation data and use_gp_model_for_validation = TRUE ")
bst <- gpb.train(data = dtrain,
                 gp_model = gp_model,
                 nrounds = 5,
                 learning_rate = 0.05,
                 max_depth = 6,
                 min_data_in_leaf = 5,
                 objective = "regression_l2",
                 verbose = 1,
                 valids = valids,
                 early_stopping_rounds = 5,
                 use_gp_model_for_validation = TRUE)
print(paste0("Optimal number of iterations: ", bst$best_iter,
             ", best test error: ", bst$best_score))


#--------------------Do Newton updates for tree leaves---------------
print("Training with Newton updates for tree leaves")
bst <- gpb.train(data = dtrain,
                 gp_model = gp_model,
                 nrounds = 100,
                 learning_rate = 0.05,
                 max_depth = 6,
                 min_data_in_leaf = 5,
                 objective = "regression_l2",
                 verbose = 1,
                 valids = valids,
                 early_stopping_rounds = 5,
                 use_gp_model_for_validation = FALSE,
                 leaves_newton_update = TRUE)
print(paste0("Optimal number of iterations: ", bst$best_iter,
             ", best test error: ", bst$best_score))


#--------------------GPBoostOOS algorithm: GP parameters estimated out-of-sample----------------
# Simulate data
f1d <- function(x) 1.7*(1/(1+exp(-(x-0.5)*20))+0.75*x)
set.seed(1)
n <- 1000 # number of samples
X <- matrix(runif(2*n),ncol=2)
y <- f1d(X[,1]) # mean
# Add grouped random effects
m <- 25 # number of categories / levels for grouping variable
group <- rep(1,n) # grouping variable
for(i in 1:m) group[((i-1)*n/m+1):(i*n/m)] <- i
sigma2_1 <- 1^2 # random effect variance
sigma2 <- 0.1^2 # error variance
# incidence matrix relating grouped random effects to samples
Z1 <- model.matrix(rep(1,n) ~ factor(group) - 1)
b1 <- sqrt(sigma2_1) * rnorm(m) # simulate random effects
eps <- Z1 \%*\% b1
xi <- sqrt(sigma2) * rnorm(n) # simulate error term
y <- y + eps + xi # add random effects and error to data
# Create random effects model and dataset
gp_model <- GPModel(group_data = group)
dtrain <- gpb.Dataset(X, label = y)
params <- list(learning_rate = 0.05,
               max_depth = 6,
               min_data_in_leaf = 5,
               objective = "regression_l2",
               has_gp_model =TRUE)
# Stage 1: run cross-validation to (i) determine to optimal number of iterations
#           and (ii) to estimate the GPModel on the out-of-sample data
cvbst <- gpb.cv(params = params,
                data = dtrain,
                gp_model = gp_model,
                nrounds = 100,
                nfold = 4,
                eval = "l2",
                early_stopping_rounds = 5,
                use_gp_model_for_validation = FALSE,
                fit_GP_cov_pars_OOS = TRUE)
print(paste0("Optimal number of iterations: ", cvbst$best_iter))
# Fitted model (note: ideally, one would have to find the optimal combination of 
#               other tuning parameters such as the learning rate, tree depth, etc.)
summary(gp_model)
# Stage 2: Train tree-boosting model while holding the GPModel fix
bst <- gpb.train(data = dtrain,
                 gp_model = gp_model,
                 nrounds = cvbst$best_iter,
                 learning_rate = 0.05,
                 max_depth = 6,
                 min_data_in_leaf = 5,
                 objective = "regression_l2",
                 verbose = 0,
                 train_gp_model_cov_pars = FALSE)
# The GPModel has not changed:
summary(gp_model)
}
}
